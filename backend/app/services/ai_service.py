# e:\Vs_Project\Novel_asisit\backend\app\services\ai_service.py
import json
import re
from functools import lru_cache
from typing import Any, Dict, List

from app.adapters.tools import load_default_toolset
from app.agents import AgentOrchestrator, ContextAgent, PlannerAgent
from app.agents.shared import PlannerRequest, PlannerResult
from app.prompts.prompt_manager import get_prompt, get_system_prompt
from app.services.llm_client import LLMClientError, call_chat_completion
from app.utils.logger import ai_logger, log_ai_call


@lru_cache(maxsize=1)
def _get_orchestrator() -> AgentOrchestrator:
    planner = PlannerAgent()
    context_agent = ContextAgent()
    tools = load_default_toolset()
    return AgentOrchestrator(planner, context_agent, tools)

def process_chapter_with_ai(chapter_content: str, user_prompt: str) -> str | None:
    """Use the orchestrator to process a chapter with a refinement instruction."""

    result = generate_content_with_intent(
        "chapter_process",
        user_prompt=user_prompt,
        metadata={
            "chapter_content": chapter_content,
            "instruction": user_prompt,
        },
    )
    processed = extract_generated_content(result)
    if processed:
        return processed

    if result.steps:
        last_output = result.steps[-1].output
        if isinstance(last_output, dict):
            processed = last_output.get("result") or last_output.get("content")
            if isinstance(processed, str) and processed.strip():
                return processed
    return None

def generate_chapter_summary(chapter_content, *, title: str | None = None) -> str | None:
    """Use the orchestrator to generate a chapter summary."""

    result = generate_content_with_intent(
        "chapter_summary",
        user_prompt=title or "",
        metadata={
            "chapter_content": chapter_content,
            "title": title or "",
        },
    )
    summary = extract_generated_content(result)
    if summary:
        return summary

    # å½“ä½¿ç”¨å†…å®¹ç”Ÿæˆå·¥å…·æ—¶ï¼Œæ‘˜è¦å¯èƒ½å‡ºç°åœ¨æœ€åä¸€æ­¥çš„ output ä¸­
    if result.steps:
        last_output = result.steps[-1].output
        if isinstance(last_output, dict):
            summary = last_output.get("summary") or last_output.get("chapters")
            if isinstance(summary, list) and summary:
                return summary[-1].get("content")
            if isinstance(summary, str) and summary.strip():
                return summary
            content = last_output.get("content")
            if isinstance(content, str) and content.strip():
                return content
    return None

def analyze_chapter_characters(chapter_content: str):
    result = generate_content_with_intent(
        "character_analysis",
        user_prompt="",
        metadata={
            "chapter_content": chapter_content,
        },
    )
    characters = result.artifacts.get("characters") if result else None
    if isinstance(characters, list) and characters:
        return characters

    for step in reversed(result.steps if result else []):
        data = step.output.get("characters")
        if isinstance(data, list) and data:
            return data

    return [
        {
            "name": "æœªçŸ¥äººç‰©",
            "description": "äººç‰©åˆ†æå¤±è´¥ï¼Œè¯·é‡è¯•",
            "actions": ["æ— æ³•è·å–è¡ŒåŠ¨ä¿¡æ¯"],
        }
    ]

def analyze_prompt_for_chapters(prompt):
    # æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°çš„promptç»“æ„æ¯”è¾ƒç®€å•ï¼Œç”¨æˆ·è¾“å…¥ç›´æ¥ä½œä¸ºå†…å®¹ï¼Œåªæœ‰ä¸€ä¸ªå›ºå®šçš„ç³»ç»Ÿæç¤ºã€‚
    # ä¹Ÿå¯ä»¥è€ƒè™‘å°† "ä½ æ˜¯ä¸€ä¸ª..." è¿™éƒ¨åˆ†ä¹Ÿæ¨¡æ¿åŒ–ï¼Œä½†ç›®å‰ä¿æŒåŸæ ·ä»¥æ±‚ç®€æ´ã€‚
    system_prompt = get_system_prompt('predict_chapters_system')
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}]
    try:
        response = call_chat_completion(messages, temperature=0.3, max_tokens=100)
    except LLMClientError as exc:
        ai_logger.warning(f"âš ï¸ ç« èŠ‚æ•°é‡é¢„æµ‹å¤±è´¥: {exc}")
        return 10
    if response and 'choices' in response and len(response['choices']) > 0:
        try:
            content = response['choices'][0]['message']['content']
            chapter_count = re.search(r'(\d+)\s*ä¸ªç« èŠ‚', content)
            if chapter_count: return int(chapter_count.group(1))
            else: return 10
        except (json.JSONDecodeError, ValueError) as e: print(f"è§£æç« èŠ‚æ•°é‡é”™è¯¯: {e}")
    return 10

def generate_content_with_intent(
    intent: str,
    user_prompt: str,
    *,
    context_text: str = "",
    context_chapters: List[Dict[str, Any]] | None = None,
    metadata: Dict[str, Any] | None = None,
) -> PlannerResult:
    """æ ¹æ®æ„å›¾ç”Ÿæˆå†…å®¹ï¼ˆä½¿ç”¨ä»£ç†ç¼–æ’ï¼‰"""

    orchestrator = _get_orchestrator()
    metadata_payload: Dict[str, Any] = {
        "context_chapters": context_chapters or [],
    }
    if metadata:
        metadata_payload.update(metadata)
    request = PlannerRequest(
        intent=intent,
        prompt=user_prompt,
        context=context_text or "",
        metadata=metadata_payload,
    )
    return orchestrator.run(request)


def extract_generated_content(result: PlannerResult) -> str:
    """ä»ç¼–æ’ç»“æœä¸­æå–æœ€ç»ˆæ–‡æœ¬å†…å®¹ã€‚"""

    if not result:
        return ""

    generated = result.artifacts.get("generated_content")
    if isinstance(generated, str) and generated.strip():
        return generated

    for step in reversed(result.steps):
        content = step.output.get("content")
        if isinstance(content, str) and content.strip():
            return content

    return ""


def extract_generated_chapters(result: PlannerResult) -> List[Dict[str, Any]]:
    """ä»ç¼–æ’ç»“æœä¸­æå–ç« èŠ‚ç»“æ„ã€‚"""

    if not result:
        return []

    chapters = result.artifacts.get("chapters")
    if isinstance(chapters, list):
        return chapters

    for step in reversed(result.steps):
        candidate = step.output.get("chapters")
        if isinstance(candidate, list):
            return candidate

    return []


def _legacy_generate_with_intent(
    intent: str,
    prompt: str,
    context: str,
    context_chapters: List[Dict[str, Any]],
) -> str:
    result = generate_content_with_intent(
        intent,
        prompt,
        context_text=context,
        context_chapters=context_chapters,
    )
    output = extract_generated_content(result)
    return output or "AIå“åº”ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ã€‚"


# ä¿ç•™æ—§çš„å‡½æ•°ä»¥ä¿æŒå‘åå…¼å®¹
def generate_novel_content(prompt, context, context_chapters):
    return _legacy_generate_with_intent("novel_generation", prompt, context, context_chapters)


def generate_plot_design(prompt, context, context_chapters):
    return _legacy_generate_with_intent("plot_design", prompt, context, context_chapters)


def generate_full_novel(prompt, context, context_chapters):
    return _legacy_generate_with_intent("novel_generation", prompt, context, context_chapters)

def classify_user_intent(user_input):
    """åˆ¤æ–­ç”¨æˆ·è¾“å…¥çš„å…·ä½“æ„å›¾ç±»å‹"""
    system_prompt = get_system_prompt('classify_intent_system')
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]
    
    ai_logger.info(f'ğŸ” å¼€å§‹åˆ†ç±»ç”¨æˆ·æ„å›¾ï¼Œè¾“å…¥é•¿åº¦: {len(user_input)} å­—ç¬¦')
    ai_logger.info(f'ğŸ“ è¾“å…¥å‰100å­—: {user_input[:100]}...')
    
    try:
        response = call_chat_completion(messages, temperature=0.1, max_tokens=20)
    except LLMClientError as exc:
        ai_logger.warning(f"âš ï¸ æ„å›¾åˆ†ç±»æ¥å£è°ƒç”¨å¤±è´¥: {exc}")
        response = None

    if response and response.get('choices'):
        intent = response['choices'][0]['message']['content'].strip().lower()
        ai_logger.info(f'ğŸ¯ æ„å›¾åˆ†ç±»åŸå§‹ç»“æœ: "{intent}"')
        
        # è¿”å›å…·ä½“çš„æ„å›¾ç±»å‹
        if 'chat' in intent:
            ai_logger.info('ğŸ’¬ åˆ¤å®šä¸º: æ™®é€šå¯¹è¯')
            return 'chat'
        elif 'plot_design' in intent or 'design' in intent:
            ai_logger.info('ğŸ“ åˆ¤å®šä¸º: å‰§æƒ…è®¾è®¡')
            return 'plot_design'
        elif 'novel_generation' in intent or 'generation' in intent:
            ai_logger.info('âœï¸ åˆ¤å®šä¸º: å°è¯´ç”Ÿæˆ')
            return 'novel_generation'
    
    # é»˜è®¤è®¤ä¸ºæ˜¯å°è¯´ç”Ÿæˆ
    ai_logger.warning('âš ï¸ æ— æ³•æ˜ç¡®åˆ¤å®šæ„å›¾ï¼Œé»˜è®¤ä¸º: å°è¯´ç”Ÿæˆ')
    return 'novel_generation'

def general_chat(user_input):
    """å¤„ç†æ™®é€šå¯¹è¯"""
    system_prompt = get_system_prompt('general_chat_system')
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]
    
    try:
        response = call_chat_completion(messages, temperature=0.7, max_tokens=2000)
    except LLMClientError as exc:
        ai_logger.error(f"âŒ æ™®é€šå¯¹è¯è°ƒç”¨å¤±è´¥: {exc}")
        response = None

    if response and response.get('choices'):
        return response['choices'][0]['message']['content']
    
    return "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›å¤ã€‚è¯·ç¨åå†è¯•ã€‚"
